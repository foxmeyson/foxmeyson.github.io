<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-02-24T20:11:54+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Evgenii Zhuravlev - DevOps Engineer</title><subtitle>Personal blog about DevOps, Kubernetes, Linux, Clouds infrastructures, CI/CD, Terraform, Ansible, and other.</subtitle><author><name>Evgenii Zhuravlev</name></author><entry><title type="html">Recovery DB in Zalando postgres operator in Kubernetes from S3</title><link href="http://localhost:4000/kubernetes/recovery-DBs-in-Zalando-postgres-operator/" rel="alternate" type="text/html" title="Recovery DB in Zalando postgres operator in Kubernetes from S3" /><published>2025-02-24T00:00:00+01:00</published><updated>2025-02-24T00:00:00+01:00</updated><id>http://localhost:4000/kubernetes/recovery-DBs-in-Zalando-postgres-operator</id><content type="html" xml:base="http://localhost:4000/kubernetes/recovery-DBs-in-Zalando-postgres-operator/"><![CDATA[<p><img src="/assets/images/posts/zalando-postgres-operator-restore-DBs/zalando-posgres-operator-banner.webp" alt="banner" /></p>

<p>While working with the Zalando Postgres Operator in Kubernetes, I encountered a significant challenge: there is no well-documented, out-of-the-box method for restoring a database from an S3 backup. The operator itself is a great tool that simplifies PostgreSQL deployment and management in Kubernetes, but when it comes to recovery, the process is not as straightforward as one might expect.</p>

<p>This guide is the result of my research, hands-on experience, and an issue I raised on GitHub regarding database recovery in Zalando’s Postgres Operator (<a href="https://github.com/zalando/postgres-operator/issues/1395">issue #1395</a>). Here, I document a working solution to recover a PostgreSQL cluster from S3, outlining the necessary steps and configurations.</p>

<p>If you’re facing a similar issue, this article should help you navigate the recovery process efficiently. Let’s dive in. 🚀</p>

<h4 id="1-do-not-touch-bucket-with-wal-old-cluster">1) Do not touch bucket with WAL old cluster.</h4>

<p>Whatch in configmap current directory for WAL:</p>

<p>Here and further: alias <strong>k = kubectl</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k get <span class="nt">-n</span> <span class="nv">$NAMESPACE</span> configmap <span class="nt">-o</span> yaml
<span class="nt">---</span>
apiVersion: v1
items:
- apiVersion: v1
  data:
    ...
    WALG_S3_PREFIX: s3://bucket/wal_for_OLD &lt;<span class="o">=============</span> current directory <span class="k">for </span>WAL <span class="o">!!!</span> Do not <span class="nb">touch </span>it <span class="k">in </span>storage <span class="o">!!!</span>
    ...
</code></pre></div></div>

<h4 id="2-create-new-directory-for-wal-in-storage">2) Create new directory for WAL in storage</h4>
<p>In S3 provider for you account in buscket create <strong>new</strong> directory for <strong>new</strong> WAL.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>You need create directory **WAL_NEW_CLUSTER**
</code></pre></div></div>

<p><img src="/assets/images/posts/zalando-postgres-operator-restore-DBs/zalando-posgres-operator-sheme.png" alt="image" /></p>

<h4 id="3-editing-configmap">3) Editing configmap</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~/git/deploy-zalando-operator/
git pull
vim pg-pod-configmap.yaml
</code></pre></div></div>

<p>add / change:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>WALG_S3_PREFIX: s3://bucket/wal_for_OLD &lt;<span class="o">===================</span> it was, need to comment out
<span class="nt">---</span>
WALG_S3_PREFIX: s3://bucket/wal_for_NEW &lt;<span class="o">============</span> became
CLONE_USE_WALG_RESTORE: <span class="s2">"true"</span> &lt;<span class="o">============</span> became
</code></pre></div></div>

<h4 id="4-apply-configmap-in-k8s">4) Apply configmap in K8s</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k apply <span class="nt">-n</span> <span class="nv">$NAMESPACE</span>  <span class="nt">-f</span> ~/git/deploy-zalando-operator/pg-pod-configmap.yaml
</code></pre></div></div>

<h4 id="5-editing-manifest-cluster">5) Editing manifest cluster</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~/git/deploy-zalando-operator/
git pull
vim zalando-cluster.yaml
</code></pre></div></div>

<p>add section:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  clone:
    cluster: old-cluster-name
    s3_access_key_id: access_key_id
    s3_endpoint: endpoint
    s3_secret_access_key: secret_access_key
    s3_wal_path: s3://bucket/wal_for_OLD <span class="c"># bucket OLD cluster, from which we will recover.</span>
    timestamp: <span class="s2">"2021-01-21T23:49:03+03:00"</span> <span class="c"># timezone required (offset relative to UTC, see RFC 3339 section 5.6)</span>
</code></pre></div></div>

<h4 id="6-check-and-apply-cluster-manifest-in-k8s">6) Check and apply cluster manifest in K8s</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k apply <span class="nt">-n</span> <span class="nv">$NAMESPACE</span> <span class="nt">-f</span> ~/git/deploy-zalando-operator/zalando-cluster.yaml <span class="nt">--server-dry-run</span>
k apply <span class="nt">-n</span> <span class="nv">$NAMESPACE</span> <span class="nt">-f</span> ~/git/deploy-zalando-operator/zalando-cluster.yaml
</code></pre></div></div>

<h4 id="7-log-pattern-cluster-can-take-a-long-time-to-up-up-to-10-minutes-on-the-test-it-may-depend-on-the-size-of-the-base">7) Log pattern (cluster can take a long time to up (up to 10 minutes on the test, it may depend on the size of the base):</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k logs <span class="nt">-n</span> <span class="nv">$NAMESPACE</span> pg-pod-<span class="k">***</span> <span class="nt">-f</span> <span class="o">(</span>only leader<span class="o">)</span>
...
021-03-05 01:30:33,855 - bootstrapping - INFO - Writing to file /run/etc/wal-e.d/env-clone-old-cluster-name/TMPDIR
2021-03-05 01:30:33,855 - bootstrapping - INFO - Configuring standby-cluster
2021-03-05 01:30:33,855 - bootstrapping - INFO - Configuring patroni
2021-03-05 01:30:33,871 - bootstrapping - INFO - Writing to file /home/postgres/postgres.yml
2021-03-05 01:30:33,871 - bootstrapping - INFO - Configuring crontab
2021-03-05 01:30:33,871 - bootstrapping - INFO - Skipping creation of renice cron job due to lack of SYS_NICE capability
2021-03-05 01:30:33,881 - bootstrapping - INFO - Configuring certificate
2021-03-05 01:30:33,881 - bootstrapping - INFO - Generating ssl certificate
2021-03-05 01:30:33,937 - bootstrapping - INFO - Configuring pam-oauth2
2021-03-05 01:30:33,937 - bootstrapping - INFO - No PAM_OAUTH2 configuration was specified, skipping
2021-03-05 01:30:33,937 - bootstrapping - INFO - Configuring pgqd
2021-03-05 01:30:33,938 - bootstrapping - INFO - Configuring log
2021-03-05 01:30:35,355 INFO: No PostgreSQL configuration items changed, nothing to reload.
2021-03-05 01:30:35,363 INFO: Lock owner: None<span class="p">;</span> I am pg-pod-0
2021-03-05 01:30:35,531 INFO: trying to bootstrap a new cluster
2021-03-05 01:30:35,532 INFO: Running custom bootstrap script: envdir <span class="s2">"/run/etc/wal-e.d/env-clone-old-cluster-name"</span> python3 /scripts/clone_with_wale.py <span class="nt">--recovery-target-time</span><span class="o">=</span><span class="s2">"2021-03-03T23:49:03+03:00"</span>
2021-03-05 01:30:35,746 INFO: cloning cluster old-cluster-name using wal-g backup-fetch /home/postgres/pgdata/pgroot/data base_000000010000000000000006
INFO: 2021/03/05 01:30:35.943395 Finished decompression of part_004.tar.br
INFO: 2021/03/05 01:30:35.943416 Finished extraction of part_004.tar.br
INFO: 2021/03/05 01:30:37.269663 Finished extraction of part_001.tar.br
INFO: 2021/03/05 01:30:37.269967 Finished decompression of part_001.tar.br
INFO: 2021/03/05 01:30:37.481460 Finished decompression of part_002.tar.br
INFO: 2021/03/05 01:30:37.481464 Finished extraction of part_002.tar.br
INFO: 2021/03/05 01:30:37.491029 Finished decompression of pg_control.tar.br
INFO: 2021/03/05 01:30:37.491047 Finished extraction of pg_control.tar.br
INFO: 2021/03/05 01:30:37.491056 
Backup extraction complete.
2021-03-05 01:30:37,724 maybe_pg_upgrade INFO: No PostgreSQL configuration items changed, nothing to reload.
2021-03-05 01:30:38 UTC <span class="o">[</span>129]: <span class="o">[</span>1-1] 604189be.81 0     LOG:  Auto detecting pg_stat_kcache.linux_hz parameter...
2021-03-05 01:30:38 UTC <span class="o">[</span>129]: <span class="o">[</span>2-1] 604189be.81 0     LOG:  pg_stat_kcache.linux_hz is <span class="nb">set </span>to 1000000
2021-03-05 01:30:38 UTC <span class="o">[</span>129]: <span class="o">[</span>3-1] 604189be.81 0     LOG:  listening on IPv4 address <span class="s2">"0.0.0.0"</span>, port 5432
2021-03-05 01:30:38 UTC <span class="o">[</span>129]: <span class="o">[</span>4-1] 604189be.81 0     LOG:  could not create IPv6 socket <span class="k">for </span>address <span class="s2">"::"</span>: Address family not supported by protocol
2021-03-05 01:30:38 UTC <span class="o">[</span>129]: <span class="o">[</span>5-1] 604189be.81 0     LOG:  listening on Unix socket <span class="s2">"/var/run/postgresql/.s.PGSQL.5432"</span>
2021-03-05 01:30:38,105 INFO: postmaster <span class="nv">pid</span><span class="o">=</span>129
2021-03-05 01:30:38 UTC <span class="o">[</span>129]: <span class="o">[</span>6-1] 604189be.81 0     LOG:  redirecting log output to logging collector process
2021-03-05 01:30:38 UTC <span class="o">[</span>129]: <span class="o">[</span>7-1] 604189be.81 0     HINT:  Future log output will appear <span class="k">in </span>directory <span class="s2">"../pg_log"</span><span class="nb">.</span>
/var/run/postgresql:5432 - rejecting connections
/var/run/postgresql:5432 - rejecting connections
/var/run/postgresql:5432 - rejecting connections
/var/run/postgresql:5432 - accepting connections
2021-03-05 01:30:40,206 INFO: establishing a new patroni connection to the postgres cluster
2021-03-05 01:30:40,285 INFO: waiting <span class="k">for </span>end of recovery after bootstrap
2021-03-05 01:30:50,277 INFO: waiting <span class="k">for </span>end of recovery after bootstrap
2021-03-05 01:31:00,270 INFO: waiting <span class="k">for </span>end of recovery after bootstrap
2021-03-05 01:31:10,275 INFO: waiting <span class="k">for </span>end of recovery after bootstrap
2021-03-05 01:31:20,285 INFO: waiting <span class="k">for </span>end of recovery after bootstrap
2021-03-05 01:31:30,270 INFO: waiting <span class="k">for </span>end of recovery after bootstrap
2021-03-05 01:31:40,275 INFO: waiting <span class="k">for </span>end of recovery after bootstrap
...
SET
DO
DO
DO
...
ALTER EXTENSION
ALTER POLICY
REVOKE
GRANT
...
2021-03-05 01:34:34.175 - /scripts/postgres_backup.sh - I was called as: /scripts/postgres_backup.sh /home/postgres/pgdata/pgroot/data
2021-03-05 01:34:34.447 - /scripts/postgres_backup.sh - producing a new backup
INFO: 2021/03/05 01:34:34.596538 Couldn<span class="s1">'t find previous backup. Doing full backup.
INFO: 2021/03/05 01:34:34.617491 Calling pg_start_backup()
2021-03-05 01:34:35.133 35 LOG Starting pgqd 3.3
2021-03-05 01:34:35.133 35 LOG auto-detecting dbs ...
INFO: 2021/03/05 01:34:35.154913 Walking ...
INFO: 2021/03/05 01:34:35.155188 Starting part 1 ...
INFO: 2021/03/05 01:34:35.155307 Starting part 2 ...
2021-03-05 01:34:42,276 INFO: Lock owner: pg-pod-0; I am pg-pod-0
2021-03-05 01:34:42,408 INFO: no action.  i am the leader with the lock
INFO: 2021/03/05 01:34:51.222156 Finished writing part 2.
INFO: 2021/03/05 01:34:51.222175 Starting part 3 ...
2021-03-05 01:34:52,276 INFO: Lock owner: pg-pod-0; I am pg-pod-0
2021-03-05 01:34:52,382 INFO: no action.  i am the leader with the lock
2021-03-05 01:35:02,277 INFO: Lock owner: pg-pod-0; I am pg-pod-0
2021-03-05 01:35:02,388 INFO: no action.  i am the leader with the lock
2021-03-05 01:35:05.162 35 LOG {ticks: 0, maint: 0, retry: 0}
INFO: 2021/03/05 01:35:10.928677 Finished writing part 3.
INFO: 2021/03/05 01:35:10.928696 Starting part 4 ...
2021-03-05 01:35:12,277 INFO: Lock owner: pg-pod-0; I am pg-pod-0
2021-03-05 01:35:12,395 INFO: no action.  i am the leader with the lock
</span></code></pre></div></div>

<h4 id="8-replace-credentials-for-bd-in-k8s">8) Replace credentials for BD in K8S</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get secret root.old-cluster-name.credentials.postgresql.acid.zalan.do <span class="nt">-n</span> <span class="nv">$NAMESPACE</span> <span class="nt">--export</span> <span class="nt">-o</span> yaml | kubectl replace <span class="nt">-n</span> <span class="nv">$NAMESPACE</span> <span class="nt">-f</span> -
</code></pre></div></div>

<h4 id="9-delete-section-clone-in-cluster-manifest-in-k8s">9) Delete section “clone” in cluster manifest in K8s</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k edit <span class="nt">-n</span> <span class="nv">$NAMESPACE</span> postgresqls.acid.zalan.do
<span class="nt">---</span>
<span class="c">#  clone: &lt;================================== comment out this section</span>
<span class="c">#    cluster: old-cluster-name</span>
<span class="c">#    s3_access_key_id: access_key_id</span>
<span class="c">#    s3_endpoint: endpoint</span>
<span class="c">#    s3_secret_access_key: secret_access_key</span>
<span class="c">#    s3_wal_path: s3://bucket/wal_for_OLD # bucket OLD cluster, from which we will recover.</span>
<span class="c">#    timestamp: "2021-01-21T23:49:03+03:00" # timezone required (offset relative to UTC, see RFC 3339 section 5.6)</span>
</code></pre></div></div>

<h4 id="10-restart-all-pods-in-namespace-with-apps-this-is-necessary-in-order-for-the-pods-to-re-read-the-secrets-of-the-base-without-deleting-the-pods-at-the-moment-this-mechanism-does-not-work-in-kubernetes-understand-what-you-are-doing-">10) “Restart” all pods in namespace with apps (This is necessary in order for the pods to re-read the secrets of the base, without deleting the pods at the moment this mechanism does not work in Kubernetes, understand what you are doing. )</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl delete <span class="nt">--all</span> pods <span class="nt">--namespace</span><span class="o">=</span><span class="nv">$NAMESPACE</span>
</code></pre></div></div>

<h4 id="11-done-you-are-amazing-">11) Done. You are amazing =)</h4>

<h1 id="plan-to-recovery-from-sql-backup-this-variant-recovery-is-possible-only-in-a-newly-deployed-cluster-variant-for-whisout-redeploy-cluster-is-below-do-not-use-for-recovery-in-existing-cluster-you-need-sql-backup-for-recovery-db">Plan to recovery from SQL backup. This variant recovery is possible only in a newly deployed cluster (Variant for whisout redeploy cluster is below). Do not use for recovery in existing cluster! You need SQL backup for recovery DB:</h1>

<h1 id="important-briefly">Important briefly</h1>

<p><strong>uid cluster for operator</strong>  = <strong>K8s uid</strong> from manifest postgres operator, you can find this field in the metadata of the source cluster:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kg <span class="nt">-n</span> <span class="nv">$NAMESPACE</span> postgresqls.acid.zalan.do <span class="nt">-o</span> yaml
<span class="nt">---</span>
apiVersion: acid.zalan.do/v1
kind: postgresql
metadata:
  name: acid-test-cluster
  uid: efd12e58-5786-11e8-b5a7-06148230260c &lt;<span class="o">=====================</span> 
</code></pre></div></div>

<h1 id="more-details">More details</h1>

<p><strong>“initialize” for patroni</strong> = <strong>“initialize” field</strong> from manifest endpoint, you can find this field in the annotations of the endpoint:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kg <span class="nt">-n</span> <span class="nv">$NAMESPACE</span> ep <span class="nv">$CLUSTER_NAME</span><span class="nt">-config</span> <span class="nt">-o</span> yaml     
<span class="nt">---</span>
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    config: ...
    initialize: <span class="s2">"6935500285706907727"</span> &lt;<span class="o">=====================</span> patroni cluster <span class="nb">id</span>
</code></pre></div></div>

<p>Show status cluster:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kg <span class="nt">-n</span> <span class="nv">$NAMESPACE</span> postgresql.acid.zalan.do/<span class="nv">$CLUSTER_NAME</span> <span class="nt">-o</span> yaml
</code></pre></div></div>

<p>We are interested in the section:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>annotations:
   kubectl.kubernetes.io/last-applied-configuration: |
      ...
</code></pre></div></div>
<p>In this section we see current status cluster.</p>

<h1 id="documentation-links-that-can-help">Documentation (links) that can help:</h1>

<p>https://postgres-operator.readthedocs.io/en/latest/reference/cluster_manifest/</p>

<p>https://github.com/zalando/postgres-operator</p>

<p>https://patroni.readthedocs.io/en/latest/</p>

<p>https://patroni.readthedocs.io/en/latest/kubernetes.html</p>

<p>https://postgres-operator.readthedocs.io/en/latest/user/#clone-from-s3</p>

<p>https://github.com/zalando/postgres-operator/issues/1279#issuecomment-783574620</p>

<p>https://github.com/zalando/postgres-operator/issues/1391</p>

<h1 id="-dangerous-zone-">!!! Dangerous zone !!!</h1>
<blockquote>
  <p>Be very confident in what you are doing, ask senior DevOps. Any responsibility for using these commands rests with you. See Denial of responsibility</p>
</blockquote>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k delete <span class="nt">-n</span> <span class="nv">$NAMESPACE</span> postgresqls.acid.zalan.do <span class="nt">--all</span> <span class="c"># delete all cluster PG</span>
<span class="nt">---</span>
k delete <span class="nt">-n</span> <span class="nv">$NAMESPACE</span> pod <span class="nv">$POD_NAME</span> <span class="c"># delete pod</span>
<span class="nt">---</span>
k delete <span class="nt">-n</span> <span class="nv">$NAMESPACE</span> configmap <span class="nv">$CM_NAME</span> <span class="c"># delete configmap</span>
<span class="nt">---</span>
k delete <span class="nt">-n</span> <span class="nv">$NAMESPACE</span> replicaset <span class="nv">$CM_NAME</span> <span class="c"># delete replicaset. May come in handy if the pods cluster get stuck on loop in Terminate &lt;-&gt; Init</span>
</code></pre></div></div>]]></content><author><name>Evgenii Zhuravlev</name></author><category term="Kubernetes" /><category term="Kubernetes" /><category term="PostgreSQL" /><category term="Helm" /><category term="S3" /><summary type="html"><![CDATA[]]></summary></entry></feed>